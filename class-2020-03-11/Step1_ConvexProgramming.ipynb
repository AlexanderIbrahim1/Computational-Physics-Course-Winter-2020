{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Convex\n",
    "using GLPK\n",
    "using Printf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to convex optimization\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{Eq:Program}\n",
    "  \\begin{array}{rll}\n",
    "    \\text{minimize} & f (\\vec{x}) & \\\\\n",
    "    \\text{over} & \\vec{x} \\in R^n & \\\\\n",
    "    & g_j (\\vec{x}) \\leq 0 & j \\in \\mathcal{J}\\\\\n",
    "    & h_k (\\vec{x}) = 0 & k \\in \\mathcal{K}\n",
    "  \\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "where all $f$, $g_j$ and $h_k$ are convex; for $f(\\vec{x})$:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\label{Eq:Convex}\n",
    "  f (\\alpha \\vec{x}_1 + (1 - \\alpha) \\vec{x}_2) \\leqslant  \\alpha f (\\vec{x}_1) + (1 - \\alpha) f (\\vec{x}_2), \\qquad \\alpha \\in [0, 1]\n",
    "\\end{equation}\n",
    "\n",
    "On left, an example of a convex function is given below, where we plot the line segment $\\color{blue}{\\alpha f (\\vec{x}_1) + (1 - \\alpha) f (\\vec{x}_2)}$ in dark blue. On the right, an example of a nonconvex function.\n",
    "\n",
    "![ConvexFun.svg](ConvexFun.svg)\n",
    "\n",
    "In the same spirit, convex sets include the line segment between any two of their points:\n",
    "![Convex_polygon_illustration1.svg](Convex_polygon_illustration1.svg)\n",
    "while convex sets do not:\n",
    "![Convex_polygon_illustration2.svg](Convex_polygon_illustration2.svg)\n",
    "\n",
    "Convex programs \\eqref{Eq:Program} have one crucial property: any local minimum is also a global minimum. In particular, this guarantees that (non pathological) optimization algorithms always find the global minimum up to numerical precision.\n",
    "\n",
    "## Linear optimization\n",
    "\n",
    "The simplest kind of convex programs is a *linear program*, where the objective and all constraints are represented by linear functions such as (technically, these are *affine functions*):\n",
    "\n",
    "\\begin{equation}\n",
    "f(\\vec{x}) = \\vec{c}^\\top \\vec{x} + d\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{Eq:LPExample}\n",
    "  \\begin{array}{rl}\n",
    "    p^\\star = \\text{minimize} & x_1 + 2 x_2 - x_3 \\\\\n",
    "    \\text{over} & \\vec{x} \\in R^3 \\\\\n",
    "    & x_1, x_2, x_3 \\ge 0 \\\\\n",
    "    & x_1 + x_2 + x_3 = 1\n",
    "  \\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "We now solve that problem using the [Convex.jl](https://github.com/JuliaOpt/Convex.jl) optimization toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(3)\n",
    "constraints = [x >= 0, sum(x) == 1]\n",
    "objective = x[1] + 2*x[2] - x[3]\n",
    "problem = Convex.minimize(objective, constraints)\n",
    "solve!(problem, GLPK.Optimizer(msg_lev=GLPK.MSG_ALL ))\n",
    "@printf(\"\\n\")\n",
    "@printf(\"Optimal objective p = %f\\n\", problem.optval)\n",
    "@printf(\"Optimal point     x = %s\\n\", x.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primal canonical form of a linear program\n",
    "\n",
    "The standard form of a linear program is as follows, with $\\vec{c} \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{m \\times n}$ and $\\vec{b} \\in \\mathbb{R}^m$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{Eq:PrimalForm}\n",
    "  \\begin{array}{rl}\n",
    "    \\text{minimize} & \\vec{c}^\\top \\vec{x} \\\\\n",
    "    \\text{over} & \\vec{x} \\in R^n \\\\\n",
    "    & A \\vec{x} = \\vec{b} \\\\\n",
    "    & \\vec{x} \\ge 0\n",
    "  \\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "The problem is fully specified by the matrix $A$ and the vectors $\\vec{b}$ and $\\vec{c}$.\n",
    "\n",
    "#### Lecture exercice\n",
    "\n",
    "Complete the program data below to solve \\eqref{Eq:LPExample}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Convex\n",
    "using GLPK\n",
    "# remember [1, 1, 1] is a 1D vector, [1; 1; 1] is a column matrix, [1 1 1] is a row matrix\n",
    "x = Variable(3)\n",
    "c = []\n",
    "A = []\n",
    "b = []\n",
    "constraints = [A*x == b, x >= 0]\n",
    "objective = dot(c, x)\n",
    "problem = Convex.minimize(objective, constraints)\n",
    "solve!(problem, GLPK.Optimizer(msg_lev=GLPK.MSG_ALL))\n",
    "@printf(\"\\n\")\n",
    "@printf(\"Optimal objective p = %f\\n\", problem.optval)\n",
    "@printf(\"Optimal point     x = %s\\n\", x.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duality\n",
    "\n",
    "We remind the primal canonical form \\eqref{Eq:PrimalForm}:\n",
    "\\begin{equation*}\n",
    "  p^\\star = \\underset{\\vec{x} \\in R^n }{\\text{minimize}} \\quad \\vec{c}^\\top \\vec{x}, \\qquad A \\vec{x} = \\vec{b}, \\qquad \\vec{x} \\ge 0,\n",
    "\\end{equation*}\n",
    "\n",
    "and introduce the notation\n",
    "\n",
    "- a variable with a $\\star$ superscript is an optimal solution (as in $p^\\star$ is the minimum of the LP),\n",
    "- a variable with a $*$ superscript is a feasible solution (as in $\\vec{x}^*$ satisfies the constraints),\n",
    "\n",
    "with the conventions\n",
    "\n",
    "- $p^\\star = + \\infty$ if the problem is infeasible,\n",
    "- $p^\\star$ is finite if the problem has an optimal (thus feasible) solution,\n",
    "- $p^\\star = -\\infty$ if the problem is unbounded, as there are feasible solutions with $\\vec{c}^\\top \\vec{x} \\rightarrow -\\infty$.\n",
    "\n",
    "We introduce the Lagrange multipliers $\\vec{y} \\in \\mathbb{R}^m$ corresponding to the constraint $A \\vec{x} = \\vec{b}$, and the Lagrange multipliers $\\vec{s} \\in \\mathbb{R}^n$ corresponding to the constraint $\\vec{x} \\ge 0$, with $\\vec{s} \\ge 0$ as they correspond to an inequality constraint. The Lagrangian function is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{Eq:Lagrangian}\n",
    "L(\\vec{x},\\vec{y},\\vec{s}) = \\vec{c}^\\top x + \\vec{y}^\\top (\\vec{b} - A \\vec{x}) - \\vec{s}^\\top \\vec{x}.\n",
    "\\end{equation}\n",
    "\n",
    "For any feasible $\\vec{x}^*$ and any $(\\vec{y}^*, \\vec{s}^*)$ with $\\vec{s} \\ge 0$, we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{Eq:LowerBound}\n",
    "L(\\vec{x}^*,\\vec{y}^*,\\vec{s}^*) = \\vec{c}^\\top x^* + (\\vec{y}^*)^\\top \\underbrace{(\\vec{b} - A \\vec{x}^*)}_{=0} - (\\vec{s}^*)^\\top \\vec{x}^* \\le \\vec{c}^\\top \\vec{x}^*\n",
    "\\end{equation}\n",
    "\n",
    "Let us now minimize $L(\\vec{x}, \\vec{y}, \\vec{s})$ over $\\vec{x}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{Eq:DualCases}\n",
    "g(\\vec{y}, \\vec{s}) = \\min_{\\vec{x}} \\vec{x}^\\top (\\vec{c} - A^\\top \\vec{y} - \\vec{s}) + \\vec{b}^\\top \\vec{y} = \\begin{cases} \\vec{b}^\\top \\vec{y}, & \\vec{c} - A^\\top \\vec{y} - \\vec{s} = 0, \\\\ -\\infty, & \\text{otherwise}. \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Note that for every $(\\vec{y}, \\vec{s})$ (with $\\vec{s} \\ge 0$ by definition), the value $g(\\vec{y}, \\vec{s})$ is a lower bound for $p^\\star$ by \\eqref{Eq:LowerBound}.\n",
    "\n",
    "In \\eqref{Eq:DualCases}, only the first case ($\\vec{c} - A^\\top \\vec{y} - \\vec{s} = 0$) leads to a nontrivial upper bound. We now maximize over such $(\\vec{y}, \\vec{s})$ to get the best lower bound.\n",
    "\n",
    "### Dual canonical form of a linear program\n",
    "\n",
    "We obtain:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{Eq:DualForm}\n",
    "  \\begin{array}{rl}\n",
    "    d^\\star = \\text{maximize} & \\vec{b}^\\top \\vec{y} \\\\\n",
    "    \\text{over} & \\vec{y} \\in \\mathbb{R}^m, \\vec{s} \\in \\mathbb{S}^n \\\\\n",
    "    & \\vec{c} - A^\\top \\vec{y} = \\vec{s} \\\\\n",
    "    & \\vec{s} \\ge 0\n",
    "  \\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "which is the dual problem. Through the Lagrangian mechanism, we have $d^\\star \\le p^\\star$ which is *weak duality*.\n",
    "\n",
    "We have the cases, for the dual problem:\n",
    "- $d^\\star = +\\infty$, dual unbounded, thus $+\\infty \\le p^\\star$ and the primal problem is infeasible,\n",
    "- $d^\\star$ is finite, the dual is feasible, the primal problem can either be feasible or infeasible (but not unbounded),\n",
    "- $d^\\star = -\\infty$, the dual problem is infeasible.\n",
    "\n",
    "For linear programs only, when $d^\\star$ is finite, then $p^\\star = d^\\star$; when $p^\\star$ is finite, also $d^\\star = p^\\star$ (this is called *strong duality*).\n",
    "\n",
    "#### Lecture exercice\n",
    "\n",
    "Fill the problem data below to solve the dual of \\eqref{Eq:LPExample}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Convex\n",
    "using GLPK\n",
    "y = Variable(1)\n",
    "s = Variable(3)\n",
    "c = []\n",
    "A = []\n",
    "b = []\n",
    "constraints = [s >= 0, s == c - transpose(A)*y]\n",
    "objective = dot(b, y)\n",
    "problem = Convex.maximize(objective, constraints)\n",
    "solve!(problem, GLPK.Optimizer(msg_lev=GLPK.MSG_ALL))\n",
    "@printf(\"\\n\")\n",
    "@printf(\"Optimal objective d = %f\\n\", problem.optval)\n",
    "@printf(\"Optimal point     y = %s\\n\", y.value)\n",
    "@printf(\"Optimal point     s = %s\\n\", s.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recovering dual variables using Convex.jl\n",
    "\n",
    "Most convex solvers solve the primal and the dual problem at the same time, as doing so lead to better robustness (the primal-dual gap $p^\\star - d^\\star$ is then a measure of convergence).\n",
    "\n",
    "Remember the primal problem:\n",
    "\\begin{equation*}\n",
    "  p^\\star = \\underset{\\vec{x} \\in R^n }{\\text{minimize}} \\quad \\vec{c}^\\top \\vec{x}, \\qquad A \\vec{x} = \\vec{b}, \\qquad \\vec{x} \\ge 0,\n",
    "\\end{equation*}\n",
    "and realize that any feasible $\\vec{x}^*$ provides an objective $p^*$ that is an upper bound on the true minimum by definition: $p^\\star \\le p^*$.\n",
    "\n",
    "The same happens for the dual problem: any feasible pair $(\\vec{y}^*, \\vec{s}^*)$ provides an objective $d^*$ that is a lower bound on the true dual maximum.\n",
    "\n",
    "Thus if we have a primal feasible solution $\\vec{x}^*$ and a dual feasible solution $(\\vec{y}^*, \\vec{s}^*)$, we sandwich the true solution $d^\\star = p^\\star$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{Eq:Sandwich}\n",
    "d^* \\le d^\\star = p^\\star \\le p^*.\n",
    "\\end{equation}\n",
    "\n",
    "Linear programming solvers try to return a feasible solution pair for both the primal and dual, with primal objective $\\tilde{p}^*$ and dual objective $\\tilde{d}^*$. It would be tempting to imagine that the true solution lies in the internal $[\\tilde{d}^*, \\tilde{p}^*]$. However the primal and dual solutions are usually *slightly* infeasible due to numerical errors, and the relation \\eqref{Eq:Sandwich} only holds approximately. Worse, it's difficult to assess the impact of such numerical errors just by looking at the problem data.\n",
    "\n",
    "Below, we provide the primal problem to Convex.jl, and recover the dual variables from the solution. Compare and contrast with the dual formulation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Convex\n",
    "using GLPK\n",
    "x = Variable(3)\n",
    "c = []\n",
    "A = []\n",
    "b = []\n",
    "constraints = [A*x == b, x >= 0]\n",
    "objective = dot(c, x)\n",
    "problem = Convex.minimize(objective, constraints)\n",
    "solve!(problem, GLPK.Optimizer(msg_lev=GLPK.MSG_ALL ))\n",
    "@printf(\"\\n\")\n",
    "@printf(\"Optimal objective d = %f\\n\", problem.optval)\n",
    "@printf(\"Optimal point     x = %s\\n\", x.value)\n",
    "@printf(\"Dual variable 1   %s\\n\", problem.constraints[1].dual)\n",
    "@printf(\"Dual variable 2   %s\\n\", problem.constraints[2].dual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
